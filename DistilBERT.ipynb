{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/final_data/train.csv')\n",
    "data = data.rename(columns={'score': 'labels'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17307 entries, 0 to 17306\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   essay_id   17307 non-null  object\n",
      " 1   full_text  17307 non-null  object\n",
      " 2   labels     17307 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 405.8+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "print(sorted(data['labels'].unique()))\n",
    "data['labels'] = data['labels'] - 1\n",
    "# print(sorted(data['labels'].unique()))\n",
    "\n",
    "num_classes = data[\"labels\"].nunique()\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sorted(data['score'].unique()))\n",
    "# data['score'] = data['score'] - 1\n",
    "# # print(sorted(data['score'].unique()))\n",
    "\n",
    "# num_classes = data[\"score\"].nunique()\n",
    "# print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['essay_id', 'full_text', 'labels'], dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 17307/17307 [00:04<00:00, 4150.17 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "# tokenizer = AutoTokenizer.from_pretrained('microsoft/deberta-v3-base')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    # Assuming 'text' and 'label' are column names in your dataset\n",
    "    result = tokenizer(examples['full_text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "    result['labels'] = examples['labels']\n",
    "    return result\n",
    "\n",
    "# Apply the function across the dataset\n",
    "print(data.columns)\n",
    "dataset = Dataset.from_pandas(data.iloc[:,1:])\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'full_text': Value(dtype='string', id=None), 'labels': Value(dtype='int64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_datasets = tokenized_dataset.train_test_split(test_size=0.2, shuffle=True, seed=46)\n",
    "train_dataset = split_datasets['train']\n",
    "eval_dataset = split_datasets['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'full_text': Value(dtype='string', id=None), 'labels': Value(dtype='int64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "{'full_text': Value(dtype='string', id=None), 'labels': Value(dtype='int64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.features)\n",
    "print(eval_dataset.features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, cohen_kappa_score\n",
    "\n",
    "# def compute_metrics(p):\n",
    "#     predictions, labels = p\n",
    "#     predictions = np.argmax(predictions, axis=1)\n",
    "#     return {\"accuracy\": accuracy_score(labels, predictions)}\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    accuracy = accuracy_score(labels.flatten(), predictions.flatten())\n",
    "    kappa = cohen_kappa_score(labels.flatten(), predictions.flatten(), weights=\"quadratic\")\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'kappa': kappa\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "class CosineAnnealingScheduler(TrainerCallback):\n",
    "    \"\"\" Custom LR Scheduler that implements a cosine annealing schedule with warmup. \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, num_warmup_steps, num_training_steps, num_cycles=0.5, last_epoch=-1):\n",
    "        self.num_warmup_steps = num_warmup_steps\n",
    "        self.num_training_steps = num_training_steps\n",
    "        self.num_cycles = num_cycles\n",
    "        self.last_epoch = last_epoch\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "    def on_step_begin(self, args, state, control, **kwargs):\n",
    "        \"\"\" Called right before a training step in the main training loop. \"\"\"\n",
    "        step = state.global_step\n",
    "        if step < self.num_warmup_steps:\n",
    "            lr_scale = float(step) / float(max(1, self.num_warmup_steps))\n",
    "        else:\n",
    "            progress = float(step - self.num_warmup_steps) / float(max(1, self.num_training_steps - self.num_warmup_steps))\n",
    "            lr_scale = max(0.0, 0.5 * (1.0 + math.cos(math.pi * self.num_cycles * 2.0 * progress)))\n",
    "        \n",
    "        for group in self.optimizer.param_groups:\n",
    "            group['lr'] = lr_scale * group['initial_lr']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainerCallback\n",
    "\n",
    "class MetricsCallback(TrainerCallback):\n",
    "    \"A callback that stores all intermediate training, validation losses and validation accuracy.\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.training_losses = []\n",
    "        self.validation_losses = []\n",
    "        self.validation_accuracy = []\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # Logs appear as a dictionary. Check if loss and eval_loss are in the dictionary and append them.\n",
    "        if 'loss' in logs:\n",
    "            self.training_losses.append(logs['loss'])\n",
    "        if 'eval_loss' in logs:\n",
    "            self.validation_losses.append(logs['eval_loss'])\n",
    "        if 'eval_accuracy' in logs:\n",
    "            self.validation_accuracy.append(logs['eval_accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer distilbert.embeddings.word_embeddings.weight is trainable.\n",
      "Layer distilbert.embeddings.position_embeddings.weight is trainable.\n",
      "Layer distilbert.embeddings.LayerNorm.weight is trainable.\n",
      "Layer distilbert.embeddings.LayerNorm.bias is trainable.\n",
      "Layer distilbert.transformer.layer.0.attention.q_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.0.attention.q_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.0.attention.k_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.0.attention.k_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.0.attention.v_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.0.attention.v_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.0.attention.out_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.0.attention.out_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.0.sa_layer_norm.weight is frozen.\n",
      "Layer distilbert.transformer.layer.0.sa_layer_norm.bias is frozen.\n",
      "Layer distilbert.transformer.layer.0.ffn.lin1.weight is frozen.\n",
      "Layer distilbert.transformer.layer.0.ffn.lin1.bias is frozen.\n",
      "Layer distilbert.transformer.layer.0.ffn.lin2.weight is frozen.\n",
      "Layer distilbert.transformer.layer.0.ffn.lin2.bias is frozen.\n",
      "Layer distilbert.transformer.layer.0.output_layer_norm.weight is frozen.\n",
      "Layer distilbert.transformer.layer.0.output_layer_norm.bias is frozen.\n",
      "Layer distilbert.transformer.layer.1.attention.q_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.1.attention.q_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.1.attention.k_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.1.attention.k_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.1.attention.v_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.1.attention.v_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.1.attention.out_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.1.attention.out_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.1.sa_layer_norm.weight is frozen.\n",
      "Layer distilbert.transformer.layer.1.sa_layer_norm.bias is frozen.\n",
      "Layer distilbert.transformer.layer.1.ffn.lin1.weight is frozen.\n",
      "Layer distilbert.transformer.layer.1.ffn.lin1.bias is frozen.\n",
      "Layer distilbert.transformer.layer.1.ffn.lin2.weight is frozen.\n",
      "Layer distilbert.transformer.layer.1.ffn.lin2.bias is frozen.\n",
      "Layer distilbert.transformer.layer.1.output_layer_norm.weight is frozen.\n",
      "Layer distilbert.transformer.layer.1.output_layer_norm.bias is frozen.\n",
      "Layer distilbert.transformer.layer.2.attention.q_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.2.attention.q_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.2.attention.k_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.2.attention.k_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.2.attention.v_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.2.attention.v_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.2.attention.out_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.2.attention.out_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.2.sa_layer_norm.weight is frozen.\n",
      "Layer distilbert.transformer.layer.2.sa_layer_norm.bias is frozen.\n",
      "Layer distilbert.transformer.layer.2.ffn.lin1.weight is frozen.\n",
      "Layer distilbert.transformer.layer.2.ffn.lin1.bias is frozen.\n",
      "Layer distilbert.transformer.layer.2.ffn.lin2.weight is frozen.\n",
      "Layer distilbert.transformer.layer.2.ffn.lin2.bias is frozen.\n",
      "Layer distilbert.transformer.layer.2.output_layer_norm.weight is frozen.\n",
      "Layer distilbert.transformer.layer.2.output_layer_norm.bias is frozen.\n",
      "Layer distilbert.transformer.layer.3.attention.q_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.3.attention.q_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.3.attention.k_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.3.attention.k_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.3.attention.v_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.3.attention.v_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.3.attention.out_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.3.attention.out_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.3.sa_layer_norm.weight is frozen.\n",
      "Layer distilbert.transformer.layer.3.sa_layer_norm.bias is frozen.\n",
      "Layer distilbert.transformer.layer.3.ffn.lin1.weight is frozen.\n",
      "Layer distilbert.transformer.layer.3.ffn.lin1.bias is frozen.\n",
      "Layer distilbert.transformer.layer.3.ffn.lin2.weight is frozen.\n",
      "Layer distilbert.transformer.layer.3.ffn.lin2.bias is frozen.\n",
      "Layer distilbert.transformer.layer.3.output_layer_norm.weight is frozen.\n",
      "Layer distilbert.transformer.layer.3.output_layer_norm.bias is frozen.\n",
      "Layer distilbert.transformer.layer.4.attention.q_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.4.attention.q_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.4.attention.k_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.4.attention.k_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.4.attention.v_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.4.attention.v_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.4.attention.out_lin.weight is frozen.\n",
      "Layer distilbert.transformer.layer.4.attention.out_lin.bias is frozen.\n",
      "Layer distilbert.transformer.layer.4.sa_layer_norm.weight is frozen.\n",
      "Layer distilbert.transformer.layer.4.sa_layer_norm.bias is frozen.\n",
      "Layer distilbert.transformer.layer.4.ffn.lin1.weight is frozen.\n",
      "Layer distilbert.transformer.layer.4.ffn.lin1.bias is frozen.\n",
      "Layer distilbert.transformer.layer.4.ffn.lin2.weight is frozen.\n",
      "Layer distilbert.transformer.layer.4.ffn.lin2.bias is frozen.\n",
      "Layer distilbert.transformer.layer.4.output_layer_norm.weight is frozen.\n",
      "Layer distilbert.transformer.layer.4.output_layer_norm.bias is frozen.\n",
      "Layer distilbert.transformer.layer.5.attention.q_lin.weight is trainable.\n",
      "Layer distilbert.transformer.layer.5.attention.q_lin.bias is trainable.\n",
      "Layer distilbert.transformer.layer.5.attention.k_lin.weight is trainable.\n",
      "Layer distilbert.transformer.layer.5.attention.k_lin.bias is trainable.\n",
      "Layer distilbert.transformer.layer.5.attention.v_lin.weight is trainable.\n",
      "Layer distilbert.transformer.layer.5.attention.v_lin.bias is trainable.\n",
      "Layer distilbert.transformer.layer.5.attention.out_lin.weight is trainable.\n",
      "Layer distilbert.transformer.layer.5.attention.out_lin.bias is trainable.\n",
      "Layer distilbert.transformer.layer.5.sa_layer_norm.weight is trainable.\n",
      "Layer distilbert.transformer.layer.5.sa_layer_norm.bias is trainable.\n",
      "Layer distilbert.transformer.layer.5.ffn.lin1.weight is trainable.\n",
      "Layer distilbert.transformer.layer.5.ffn.lin1.bias is trainable.\n",
      "Layer distilbert.transformer.layer.5.ffn.lin2.weight is trainable.\n",
      "Layer distilbert.transformer.layer.5.ffn.lin2.bias is trainable.\n",
      "Layer distilbert.transformer.layer.5.output_layer_norm.weight is trainable.\n",
      "Layer distilbert.transformer.layer.5.output_layer_norm.bias is trainable.\n",
      "Layer pre_classifier.weight is trainable.\n",
      "Layer pre_classifier.bias is trainable.\n",
      "Layer classifier.weight is trainable.\n",
      "Layer classifier.bias is trainable.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ext3/miniconda3/lib/python3.12/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, TrainingArguments, Trainer, AutoModelForSequenceClassification, AutoConfig\n",
    "\n",
    "num_epochs = 7\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",     # evaluation is done at the end of each epoch\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=num_epochs,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"kappa\"\n",
    ")\n",
    "\n",
    "# model = AutoModelForSequenceClassification.from_pretrained('microsoft/deberta-v3-base', num_labels=num_classes)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=num_classes)\n",
    "\n",
    "config = AutoConfig.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=num_classes, hidden_dropout_prob=0.30, attention_probs_dropout_prob=0.30)\n",
    "# config = AutoConfig.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=num_classes)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_config(config)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", config=config)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"distilbert.transformer.layer\") and int(name.split('.')[3]) < 5:  # Adjust the layer numbers as needed\n",
    "        param.requires_grad = False\n",
    "        print(f\"Layer {name} is frozen.\")\n",
    "    else:\n",
    "        print(f\"Layer {name} is trainable.\")\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5, weight_decay=0.1)\n",
    "# num_training_steps = 3 * len(train_dataset) // training_args.per_device_train_batch_size\n",
    "num_training_steps = num_epochs * len(train_dataset) // training_args.per_device_train_batch_size\n",
    "scheduler_callback = CosineAnnealingScheduler(optimizer, num_warmup_steps=100, num_training_steps=num_training_steps)\n",
    "metrics_callback = MetricsCallback()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    optimizers=(optimizer, None),\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[scheduler_callback, metrics_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=6, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='72' max='3031' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  72/3031 00:27 < 19:07, 2.58 it/s, Epoch 0.16/7]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:1859\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1857\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1858\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1860\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1861\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1862\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1863\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1864\u001b[0m     )\n",
      "File \u001b[0;32m/ext3/miniconda3/lib/python3.12/site-packages/transformers/trainer.py:2208\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2202\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m   2203\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   2205\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   2206\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   2207\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m-> 2208\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   2209\u001b[0m ):\n\u001b[1;32m   2210\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   2211\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[1;32m   2212\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_losses = metrics_callback.training_losses\n",
    "validation_losses = metrics_callback.validation_losses\n",
    "validation_accuracy = metrics_callback.validation_accuracy\n",
    "\n",
    "print(\"Training Losses:\", training_losses)\n",
    "print(\"Validation Losses:\", validation_losses)\n",
    "print(\"Validation Accuracy:\", validation_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(1,num_epochs+1)),validation_losses)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Validation loss\")\n",
    "plt.title(\"Validation Loss vs Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4989035,
     "sourceId": 8388118,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "DL_Pytorch",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
